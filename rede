#foi feito no google colab
#este exeplo teve como base o site : https://www.analyticsvidhya.com

!pip install  -q tensorflow
import tensorflow as tf

import torch  as tc
import matplotlib.pyplot as plt

n_entrada , n_meio, n_saida = 5, 3, 1

x = tc.randn((1,n_entrada))
y= tc.randn((1, n_saida))

w1 = tc.randn(n_entrada, n_meio)
w2 = tc.randn (n_meio, n_saida)

b1 = tc.randn((1,n_meio))
b2= tc.randn((1,n_saida))

def sigmoid_activation (z):
    return 1/(1+ tc.exp(-z))

## ativar of meio layer 
z1 = tc.mm(x, w1) + b1
a1 = sigmoid_activation(z1)


## activation (saida) of final layer 
z2 = tc.mm(a1, w2) + b2
output = sigmoid_activation(z2)

loss = y - n_saida


## function to calculate the derivative of activation
def sigmoid_delta(x):
  return x * (1 - x)


## compute derivative of error terms
delta_output = sigmoid_delta(n_saida)
delta_hidden = sigmoid_delta(a1)

d_outp = loss * delta_output
loss_h = tc.mm(d_outp, w2.t())
d_hidn = loss_h * delta_hidden


learning_rate = 0.1

w2 += tc.mm(a1.t(), d_outp) * learning_rate
w1 += tc.mm(x.t(), d_hidn) * learning_rate


b2 += d_outp.sum() * learning_rate
b1 += d_hidn.sum() * learning_rate

plt.plot() #escolha funcao 
plt.show()
